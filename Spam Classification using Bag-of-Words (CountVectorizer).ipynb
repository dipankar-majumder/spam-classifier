{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJ1DolppI1wP"
   },
   "source": [
    "##### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzDvBmA7E8Ug"
   },
   "source": [
    "##### Load Dataset to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9765,
     "status": "ok",
     "timestamp": 1752859027051,
     "user": {
      "displayName": "Dipankar Majumder",
      "userId": "18078728093841324887"
     },
     "user_tz": -330
    },
    "id": "sOz4FurDFBov",
    "outputId": "13b0dbc6-577a-4326-f850-fbd02b0246ce"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df__spam_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspam_dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df__spam_dataset\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataframe shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf__spam_dataset\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df__spam_dataset = pd.read_csv('spam_dataset.csv')\n",
    "df__spam_dataset.head()\n",
    "\n",
    "print(f'Dataframe shape: {df__spam_dataset.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K8NMzprI9ZI"
   },
   "source": [
    "##### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13586,
     "status": "ok",
     "timestamp": 1752859040640,
     "user": {
      "displayName": "Dipankar Majumder",
      "userId": "18078728093841324887"
     },
     "user_tz": -330
    },
    "id": "leW9T3e1Ir78",
    "outputId": "4ee4dfcf-675b-4f84-d7b2-0fb91b2867e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'punkt' already downloaded.\n",
      "'punkt_tab' already downloaded.\n",
      "'stopwords' already downloaded.\n",
      "Downloading 'wordnet'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dipan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dipan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'omw-1.4'...\n",
      "--- Loading preprocessed data from spam_dataset_preprocessed.csv ---\n",
      "Preprocessed data loaded successfully. ✅\n",
      "DataFrame shape: (92367, 4)\n",
      "Columns available: ['label', 'text', 'cleaned_text', 'label_encoded']\n",
      "Sample data:\n",
      "                                                text  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                        cleaned_text label  label_encoded  \n",
      "0  go point crazy available bugis n great world l...   ham              0  \n",
      "1                              ok lar joking wif oni   ham              0  \n",
      "2  free entry wkly comp win fa cup final st may t...  spam              1  \n",
      "3                    dun say early hor c already say   ham              0  \n",
      "4                nah think go usf life around though   ham              0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "from bs4 import MarkupResemblesLocatorWarning # Import the specific warning\n",
    "import unicodedata # For character normalization\n",
    "from collections import Counter # For rare word filtering\n",
    "import os # For file path operations\n",
    "import warnings # For warning management\n",
    "import emoji # For emoji handling\n",
    "import contractions # Import the contractions library\n",
    "\n",
    "\n",
    "# --- Suppress BeautifulSoup warnings ---\n",
    "# Warnings related to MarkupResemblesLocator are ignored.\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "# --- Global Configurations and NLTK Downloads ---\n",
    "\n",
    "# Define the path for the preprocessed CSV file.\n",
    "PREPROCESSED_FILE_PATH = 'spam_dataset_preprocessed.csv'\n",
    "\n",
    "# A function is defined to safely manage NLTK data downloads.\n",
    "# It checks if a resource is already available before initiating a download,\n",
    "# preventing unnecessary re-downloads and handling potential errors.\n",
    "def safe_nltk_download(resource_path, download_name):\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "        print(f\"'{download_name}' already downloaded.\")\n",
    "    except LookupError:\n",
    "        print(f\"Downloading '{download_name}'...\")\n",
    "        nltk.download(download_name)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while checking/downloading '{download_name}': {e}\")\n",
    "\n",
    "# NLTK data packages are downloaded, which are essential for text processing.\n",
    "# 'punkt' and 'punkt_tab' are used for tokenization, 'stopwords' provides a list of common words to filter,\n",
    "# 'wordnet' supports lemmatization, and 'omw-1.4' is a dependency for WordNet.\n",
    "safe_nltk_download('tokenizers/punkt', 'punkt')\n",
    "safe_nltk_download('tokenizers/punkt_tab', 'punkt_tab')\n",
    "safe_nltk_download('corpora/stopwords', 'stopwords')\n",
    "safe_nltk_download('corpora/wordnet', 'wordnet')\n",
    "safe_nltk_download('corpora/omw-1.4', 'omw-1.4')\n",
    "\n",
    "# NLTK components for text processing are initialized globally.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# A regex pattern for common emoticons is compiled globally.\n",
    "EMOTICON_PATTERN = re.compile(r'(?:[:;=X][-o*]?[DPpS\\|/\\)\\(])|(?:\\<3)')\n",
    "\n",
    "# A threshold for rare word filtering is defined globally.\n",
    "RARE_WORD_THRESHOLD = 5\n",
    "\n",
    "# The core text preprocessing function is defined.\n",
    "def preprocess_text(text):\n",
    "    # HTML tags are removed using BeautifulSoup.\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # Emojis are converted to their textual descriptions.\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # Common emoticons are replaced with a descriptive token.\n",
    "    text = EMOTICON_PATTERN.sub(r' EMOTICON ', text)\n",
    "\n",
    "    # Text is converted to lowercase.\n",
    "    text = text.lower()\n",
    "\n",
    "    # Contractions are expanded to their full forms using the 'contractions' library.\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # URLs within the text are removed.\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Unicode characters are normalized to their closest ASCII equivalents (NFKD form)\n",
    "    # and then filtered to remove non-ASCII characters, standardizing character representation.\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Escape characters (like newlines, tabs, carriage returns) are replaced with a single space.\n",
    "    # Multiple spaces are also condensed into a single space.\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Punctuation marks are eliminated from the text.\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # All numerical digits are removed from the text.\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # The text is broken down into individual words (tokens).\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Stop words are filtered out, and remaining words are lemmatized.\n",
    "    cleaned_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words: # Accessing global stop_words\n",
    "            lemma = lemmatizer.lemmatize(word) # Accessing global lemmatizer\n",
    "            cleaned_tokens.append(lemma)\n",
    "\n",
    "    # Cleaned tokens are rejoined into a single string.\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# --- Functions for Preprocessing Stages ---\n",
    "\n",
    "def perform_initial_data_checks(df):\n",
    "    # Initial DataFrame information and missing values are displayed.\n",
    "    print(\"--- Initial DataFrame Info & Missing Values (before NA/Duplicates) ---\")\n",
    "    print(df.info())\n",
    "    print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
    "    print(\"\\nOriginal label distribution:\\n\", df['label'].value_counts())\n",
    "    return df\n",
    "\n",
    "def handle_missing_and_duplicate_values(df):\n",
    "    # Rows with missing values in 'text' or 'label' are handled by dropping them.\n",
    "    initial_rows_before_na = df.shape[0]\n",
    "    df.dropna(subset=['text', 'label'], inplace=True)\n",
    "    if df.shape[0] < initial_rows_before_na:\n",
    "        print(f\"\\n{initial_rows_before_na - df.shape[0]} rows were dropped due to missing values. 🗑️\")\n",
    "\n",
    "    # Duplicate rows are identified based on the 'text' column, and duplicates are removed,\n",
    "    # retaining the first occurrence of each unique text message.\n",
    "    print(\"\\n--- Handling Duplicate Values ---\")\n",
    "    initial_rows_before_duplicates = df.shape[0]\n",
    "    df.drop_duplicates(subset=['text'], inplace=True)\n",
    "    if df.shape[0] < initial_rows_before_duplicates:\n",
    "        print(f\"\\n{initial_rows_before_duplicates - df.shape[0]} duplicate rows were removed. 🗑️\")\n",
    "    else:\n",
    "        print(\"\\nNo duplicate rows were found based on the 'text' column. 🗑️\")\n",
    "\n",
    "    print(f\"DataFrame shape after NA and duplicate removal: {df.shape}\")\n",
    "    print(\"\\nUpdated label distribution after duplicate removal:\\n\", df['label'].value_counts())\n",
    "    return df\n",
    "\n",
    "def apply_text_normalization_pipeline(df):\n",
    "    # The advanced text preprocessing function is applied to the 'text' column,\n",
    "    # now using standard .apply() for sequential processing.\n",
    "    print(\"\\n--- Applying Advanced Text Normalization (Sequential Processing) ---\")\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    print(\"\\nSample original vs. cleaned text (after initial advanced normalization):\") # Ensure we don't go out of bounds for small DFs\n",
    "    for i in range(min(5, len(df))):\n",
    "        print(f\"Original: {df['text'].iloc[i]}\")\n",
    "        print(f\"Cleaned:  {df['cleaned_text'].iloc[i]}\\n\")\n",
    "    \n",
    "    # All words from the 'cleaned_text' column are collected and tokenized.\n",
    "    all_words = []\n",
    "    for text in df['cleaned_text']:\n",
    "        all_words.extend(text.split())\n",
    "\n",
    "    # Word frequencies across the entire corpus are calculated.\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    # Rare words are identified based on a predefined threshold.\n",
    "    rare_words = {word for word, count in word_counts.items() if count < RARE_WORD_THRESHOLD}\n",
    "    print(f\"\\n--- Applying Rare Word Filtering ---\")\n",
    "    print(f\"Identified {len(rare_words)} rare words (appearing less than {RARE_WORD_THRESHOLD} times).\")\n",
    "\n",
    "    # Rare words are removed from the 'cleaned_text' column.\n",
    "    df['cleaned_text_filtered'] = df['cleaned_text'].apply(\n",
    "        lambda text: ' '.join([word for word in text.split() if word not in rare_words])\n",
    "    )\n",
    "\n",
    "    print(\"\\nSample cleaned vs. filtered text (after rare word removal):\") # Ensure we don't go out of bounds\n",
    "    for i in range(min(5, len(df))):\n",
    "        print(f\"Cleaned (before filter):  {df['cleaned_text'].iloc[i]}\")\n",
    "        print(f\"Filtered (after filter): {df['cleaned_text_filtered'].iloc[i]}\\n\")\n",
    "\n",
    "    # The main 'cleaned_text' column is updated with the filtered text, and the temporary column is dropped.\n",
    "    df['cleaned_text'] = df['cleaned_text_filtered']\n",
    "    df.drop(columns=['cleaned_text_filtered'], inplace=True)\n",
    "\n",
    "    # --- Handle empty cleaned text data ---\n",
    "    initial_rows_before_empty_check = df.shape[0]\n",
    "    # Filter out rows where 'cleaned_text' is empty or contains only whitespace\n",
    "    df['cleaned_text'].replace('', np.nan, inplace=True) # Replace empty strings with NaN\n",
    "    df.dropna(subset=['cleaned_text'], inplace=True) # Drop rows where 'cleaned_text' is NaN\n",
    "    \n",
    "    if df.shape[0] < initial_rows_before_empty_check:\n",
    "        print(f\"\\n{initial_rows_before_empty_check - df.shape[0]} rows were dropped because 'cleaned_text' became empty after preprocessing. 🗑️\")\n",
    "    else:\n",
    "        print(\"\\nNo rows had empty 'cleaned_text' after preprocessing. ✅\")\n",
    "\n",
    "    # A note on spelling correction is provided, indicating its optional and resource-intensive nature.\n",
    "    print(\"\\n--- Spelling Correction/Normalization (Optional) ---\")\n",
    "    print(\"Spelling correction is a computationally intensive step and can significantly increase processing time,\")\n",
    "    print(\"especially for large datasets. It is often omitted or applied selectively.\")\n",
    "    print(\"If desired, the 'spellchecker' library can be used, but it's not included in the main pipeline by default.\")\n",
    "    return df\n",
    "\n",
    "def apply_label_encoding(df):\n",
    "    # A LabelEncoder is instantiated for categorical label conversion.\n",
    "    le = LabelEncoder()\n",
    "    # Categorical labels in the 'label' column are transformed into numerical representations.\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "\n",
    "    # The mapping from original labels to encoded numerical values is displayed.\n",
    "    label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(f\"\\n--- Applying Label Encoding ---\")\n",
    "    print(f\"Label mapping: {label_mapping} 🏷️\")\n",
    "    return df\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "# The script checks for the existence of a preprocessed CSV file.\n",
    "if os.path.exists(PREPROCESSED_FILE_PATH):\n",
    "    # If the file is found, the preprocessed data is loaded directly.\n",
    "    print(f\"--- Loading preprocessed data from {PREPROCESSED_FILE_PATH} ---\")\n",
    "    df__spam_dataset = pd.read_csv(PREPROCESSED_FILE_PATH)\n",
    "    print(\"Preprocessed data loaded successfully. ✅\")\n",
    "    print(f\"DataFrame shape: {df__spam_dataset.shape}\")\n",
    "    print(\"Columns available:\", df__spam_dataset.columns.tolist())\n",
    "    print(\"Sample data:\")\n",
    "    print(df__spam_dataset[['text', 'cleaned_text', 'label', 'label_encoded']].head())\n",
    "\n",
    "else:\n",
    "    # If the preprocessed file is not found, the preprocessing pipeline is executed.\n",
    "    # It is assumed that df__spam_dataset is already loaded from 'spam_dataset.csv'\n",
    "    # in a preceding cell, containing 'text' and 'label' columns.\n",
    "    try:\n",
    "        # This checks if 'df__spam_dataset' is defined, e.g., from a previous cell where it was loaded.\n",
    "        _ = df__spam_dataset.head()\n",
    "        print(\"--- Preprocessed file not found. Starting preprocessing on existing df__spam_dataset. ---\")\n",
    "    except NameError:\n",
    "        print(\"Error: 'df__spam_dataset' is not defined. Please ensure the raw dataset is loaded (e.g., pd.read_csv('spam_dataset.csv')) before running this block. ❌\")\n",
    "        # If df__spam_dataset isn't loaded, you might uncomment a line like this for testing:\n",
    "        # df__spam_dataset = pd.read_csv('spam_dataset.csv') # Make sure this file exists in your environment\n",
    "        raise # Stop execution if the DataFrame isn't available.\n",
    "\n",
    "\n",
    "    df__spam_dataset = perform_initial_data_checks(df__spam_dataset)\n",
    "    df__spam_dataset = handle_missing_and_duplicate_values(df__spam_dataset)\n",
    "    df__spam_dataset = apply_text_normalization_pipeline(df__spam_dataset)\n",
    "    df__spam_dataset = apply_label_encoding(df__spam_dataset)\n",
    "\n",
    "    print(\"\\n--- Preprocessing Complete! 🎉 ---\")\n",
    "    print(\"Updated DataFrame Info:\")\n",
    "    print(df__spam_dataset.info())\n",
    "    print(\"\\nSample of DataFrame after preprocessing:\")\n",
    "    print(df__spam_dataset[['text', 'cleaned_text', 'label', 'label_encoded']].head())\n",
    "    print(\"\\nFinal label distribution (encoded):\\n\", df__spam_dataset['label_encoded'].value_counts())\n",
    "\n",
    "    # The newly preprocessed DataFrame is saved to a CSV file for faster future loading.\n",
    "    print(f\"\\n--- Saving preprocessed data to {PREPROCESSED_FILE_PATH} ---\")\n",
    "    try:\n",
    "        df__spam_dataset.to_csv(PREPROCESSED_FILE_PATH, index=False)\n",
    "        print(\"Preprocessed data saved successfully for faster future loading. 💾\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving preprocessed data: {e} ❌\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 92367 entries, 0 to 92366\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   label          92367 non-null  object\n",
      " 1   text           92367 non-null  object\n",
      " 2   cleaned_text   92367 non-null  object\n",
      " 3   label_encoded  92367 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df__spam_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfST8CO0_0Wn"
   },
   "source": [
    "##### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1752859044219,
     "user": {
      "displayName": "Dipankar Majumder",
      "userId": "18078728093841324887"
     },
     "user_tz": -330
    },
    "id": "zyQH58JlBTYR",
    "outputId": "f6c8fae8-f2a4-44d3-d5e8-e2287bf865da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Extraction: Bag-of-Words (CountVectorizer) ---\n",
      "CountVectorizer features or model not found. Generation will be performed.\n",
      "CountVectorizer features shape: (92367, 5000)\n",
      "CountVectorizer vocabulary size: 5000\n",
      "CountVectorizer features generated and saved to: feature_extraction_models\\count_feature_vectors.joblib\n",
      "CountVectorizer model saved to: feature_extraction_models\\count_vectorizer.joblib\n",
      "\n",
      "Bag-of-Words Feature Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib # For saving/loading models and feature matrices\n",
    "import os # For checking file existence\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# --- Assume df__spam_dataset is available from previous preprocessing steps ---\n",
    "# It is assumed that df__spam_dataset contains 'cleaned_text' and 'label_encoded' columns.\n",
    "# If running this cell independently, ensure df__spam_dataset is loaded first.\n",
    "# Example: df__spam_dataset = pd.read_csv('spam_dataset_preprocessed.csv')\n",
    "\n",
    "# --- Define paths for saving/loading ---\n",
    "# A directory for storing feature extraction models and vectors is defined.\n",
    "FEATURE_EXTRACTION_DIR = 'feature_extraction_models'\n",
    "os.makedirs(FEATURE_EXTRACTION_DIR, exist_ok=True)\n",
    "\n",
    "# File paths for the CountVectorizer model and its generated feature vectors are specified.\n",
    "COUNT_VECTORIZER_MODEL_PATH = os.path.join(FEATURE_EXTRACTION_DIR, 'count_vectorizer.joblib')\n",
    "COUNT_VECTORS_PATH = os.path.join(FEATURE_EXTRACTION_DIR, 'count_feature_vectors.joblib')\n",
    "\n",
    "# A variable to hold the generated CountVectorizer features is initialized.\n",
    "count_vectors = None\n",
    "# A variable to hold the fitted CountVectorizer model is initialized.\n",
    "count_vectorizer_model = None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "## Feature Extraction: Bag-of-Words (CountVectorizer)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Feature Extraction: Bag-of-Words (CountVectorizer) ---\")\n",
    "\n",
    "# The script checks if pre-existing CountVectorizer features and model are available.\n",
    "if os.path.exists(COUNT_VECTORS_PATH) and os.path.exists(COUNT_VECTORIZER_MODEL_PATH):\n",
    "    try:\n",
    "        # If found, the feature vectors and the fitted model are loaded.\n",
    "        count_vectors = joblib.load(COUNT_VECTORS_PATH)\n",
    "        count_vectorizer_model = joblib.load(COUNT_VECTORIZER_MODEL_PATH)\n",
    "        print(f\"CountVectorizer features loaded from: {COUNT_VECTORS_PATH}\")\n",
    "        print(f\"CountVectorizer model loaded from: {COUNT_VECTORIZER_MODEL_PATH}\")\n",
    "        print(f\"CountVectorizer features shape: {count_vectors.shape}\")\n",
    "        print(f\"CountVectorizer vocabulary size: {len(count_vectorizer_model.vocabulary_)}\")\n",
    "    except Exception as e:\n",
    "        # An error message is displayed if loading fails, and regeneration is initiated.\n",
    "        print(f\"Error loading CountVectorizer features or model: {e}. Regeneration will be performed.\")\n",
    "        count_vectors = None # Reset to None to trigger regeneration\n",
    "        count_vectorizer_model = None\n",
    "else:\n",
    "    # A message indicating that features were not found is displayed, and generation is initiated.\n",
    "    print(\"CountVectorizer features or model not found. Generation will be performed.\")\n",
    "\n",
    "# If features were not loaded (either not found or an error occurred during loading), they are generated.\n",
    "if count_vectors is None:\n",
    "    # A CountVectorizer is instantiated.\n",
    "    # 'max_features' is set to limit the vocabulary size, focusing on the most frequent words.\n",
    "    # 'ngram_range' is set to (1,3) to consider individual words (trigrams).\n",
    "    count_vectorizer_model = CountVectorizer(max_features=5000, ngram_range=(1,3))\n",
    "\n",
    "    # The vectorizer is fitted to the 'cleaned_text' data and the text is transformed into feature vectors.\n",
    "    count_vectors = count_vectorizer_model.fit_transform(df__spam_dataset['cleaned_text'])\n",
    "\n",
    "    print(f\"CountVectorizer features shape: {count_vectors.shape}\")\n",
    "    print(f\"CountVectorizer vocabulary size: {len(count_vectorizer_model.vocabulary_)}\")\n",
    "\n",
    "    try:\n",
    "        # The generated feature vectors and the fitted model are saved for future use.\n",
    "        joblib.dump(count_vectors, COUNT_VECTORS_PATH)\n",
    "        joblib.dump(count_vectorizer_model, COUNT_VECTORIZER_MODEL_PATH)\n",
    "        print(f\"CountVectorizer features generated and saved to: {COUNT_VECTORS_PATH}\")\n",
    "        print(f\"CountVectorizer model saved to: {COUNT_VECTORIZER_MODEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        # An error message is displayed if saving fails.\n",
    "        print(f\"Error saving CountVectorizer features or model: {e}\")\n",
    "else:\n",
    "    # A message indicating that generation was skipped due to successful loading is displayed.\n",
    "    print(\"CountVectorizer generation was skipped as features and model were loaded.\")\n",
    "\n",
    "print(\"\\nBag-of-Words Feature Extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFGa41BLQ975"
   },
   "source": [
    "##### Splitting the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6451,
     "status": "ok",
     "timestamp": 1752859050672,
     "user": {
      "displayName": "Dipankar Majumder",
      "userId": "18078728093841324887"
     },
     "user_tz": -330
    },
    "id": "PP-Cn5SLQ8CI",
    "outputId": "bb13ed4e-aeb7-4db6-b48d-881e14c65e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded from spam_dataset_preprocessed.csv\n",
      "CountVectorizer features loaded from: feature_extraction_models\\count_feature_vectors.joblib\n",
      "Shape of loaded features: (92367, 5000)\n",
      "\n",
      "--- Splitting Data into Training and Testing Sets ---\n",
      "Original features shape: (92367, 5000)\n",
      "Original labels shape: (92367,)\n",
      "Training features (X_train) shape: (73893, 5000)\n",
      "Testing features (X_test) shape: (18474, 5000)\n",
      "Training labels (y_train) shape: (73893,)\n",
      "Testing labels (y_test) shape: (18474,)\n",
      "\n",
      "Data splitting complete! Your data is now ready for model training. 🎉\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib # For loading the feature vectors\n",
    "import os # For path operations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Assume df__spam_dataset and count_vectors are available ---\n",
    "# If running this cell independently, ensure they are loaded/defined first.\n",
    "\n",
    "# Define paths (consistent with your previous feature extraction block)\n",
    "FEATURE_EXTRACTION_DIR = 'feature_extraction_models'\n",
    "COUNT_VECTORS_PATH = os.path.join(FEATURE_EXTRACTION_DIR, 'count_feature_vectors.joblib')\n",
    "\n",
    "# Load the preprocessed DataFrame if not already in memory\n",
    "try:\n",
    "    # Assuming the preprocessed CSV path from earlier\n",
    "    PREPROCESSED_FILE_PATH = 'spam_dataset_preprocessed.csv'\n",
    "    df__spam_dataset = pd.read_csv(PREPROCESSED_FILE_PATH)\n",
    "    print(f\"DataFrame loaded from {PREPROCESSED_FILE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Preprocessed DataFrame not found. Please ensure 'spam_dataset_preprocessed.csv' exists.\")\n",
    "    # Exit or handle error appropriately if df__spam_dataset is critical for next steps\n",
    "\n",
    "# Load the count_vectors if not already in memory\n",
    "count_vectors = None\n",
    "if os.path.exists(COUNT_VECTORS_PATH):\n",
    "    try:\n",
    "        count_vectors = joblib.load(COUNT_VECTORS_PATH)\n",
    "        print(f\"CountVectorizer features loaded from: {COUNT_VECTORS_PATH}\")\n",
    "        print(f\"Shape of loaded features: {count_vectors.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CountVectorizer features: {e}. Please ensure the feature extraction step was successful.\")\n",
    "else:\n",
    "    print(\"CountVectorizer features not found. Please run the feature extraction step first.\")\n",
    "\n",
    "\n",
    "# --- Data Splitting ---\n",
    "print(\"\\n--- Splitting Data into Training and Testing Sets ---\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# X is your feature matrix (sparse matrix from CountVectorizer)\n",
    "X = count_vectors\n",
    "\n",
    "# y is your target variable (encoded labels from the DataFrame)\n",
    "y = df__spam_dataset['label_encoded']\n",
    "\n",
    "# Perform the train-test split\n",
    "# test_size=0.20 means 20% of data for testing, 80% for training\n",
    "# random_state for reproducibility: ensures the same split every time the code is run\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y) # Stratify to maintain class distribution\n",
    "\n",
    "print(f\"Original features shape: {X.shape}\")\n",
    "print(f\"Original labels shape: {y.shape}\")\n",
    "print(f\"Training features (X_train) shape: {X_train.shape}\")\n",
    "print(f\"Testing features (X_test) shape: {X_test.shape}\")\n",
    "print(f\"Training labels (y_train) shape: {y_train.shape}\")\n",
    "print(f\"Testing labels (y_test) shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\nData splitting complete! Your data is now ready for model training. 🎉\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rii-udbgS1ly"
   },
   "source": [
    "##### Model Selection & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqqqfKncRPSN",
    "outputId": "29af8b92-21dd-4aa5-8e1f-ba8e2b351699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split verified for parallel model training.\n",
      "\n",
      "--- Parallel Model Training and Evaluation (Including Ensembles) ---\n",
      "Attempting to train 1 models in parallel using 16 cores via joblib...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   1 tasks      | elapsed:  1.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping results for Voting Classifier (Soft) due to training error.\n",
      "\n",
      "All models trained and evaluated in parallel! 🎉\n",
      "\n",
      "--- Reviewing All Model Results ---\n",
      "No models were successfully trained or evaluated.\n",
      "\n",
      "All model results saved to: feature_extraction_models\\all_model_results.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Base ML Algorithms\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier # Added SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Ensemble Learning Algorithms\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,  # Already there, but a bagging example\n",
    "    GradientBoostingClassifier, # Boosting\n",
    "    AdaBoostClassifier,       # Boosting\n",
    "    BaggingClassifier,        # General Bagging\n",
    "    VotingClassifier,         # Voting\n",
    "    StackingClassifier        # Stacking\n",
    ")\n",
    "\n",
    "# For splitting data (needed for Stacking meta-learner)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# For parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# --- IMPORTANT ASSUMPTION ---\n",
    "# This code block assumes that the following variables are already defined and available\n",
    "# from previous execution cells:\n",
    "# X_train, X_test, y_train, y_test\n",
    "# These should be the result of your train_test_split from the previous dedicated step.\n",
    "# If not, please ensure the \"Splitting the Data into Training and Testing Sets\" cell is run first.\n",
    "\n",
    "# Define paths for saving model results (consistent with previous steps)\n",
    "FEATURE_EXTRACTION_DIR = 'feature_extraction_models'\n",
    "os.makedirs(FEATURE_EXTRACTION_DIR, exist_ok=True)\n",
    "MODEL_RESULTS_PATH = os.path.join(FEATURE_EXTRACTION_DIR, 'all_model_results.joblib')\n",
    "\n",
    "# --- Safeguard for X_train, X_test, y_train, y_test (if running standalone) ---\n",
    "# This part is a safety net. In a typical Colab flow, if the previous cells ran,\n",
    "# these variables would already be in memory.\n",
    "\n",
    "# Load df__spam_dataset if not already loaded (needed for label_encoded for splitting)\n",
    "try:\n",
    "    PREPROCESSED_FILE_PATH = 'spam_dataset_preprocessed.csv'\n",
    "    df__spam_dataset = pd.read_csv(PREPROCESSED_FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"Preprocessed DataFrame not found. Please ensure 'spam_dataset_preprocessed.csv' exists and run the preprocessing step.\")\n",
    "\n",
    "# Load count_vectors if not already loaded (needed for X)\n",
    "COUNT_VECTORS_PATH = os.path.join(FEATURE_EXTRACTION_DIR, 'count_feature_vectors.joblib')\n",
    "count_vectors = None\n",
    "if os.path.exists(COUNT_VECTORS_PATH):\n",
    "    try:\n",
    "        count_vectors = joblib.load(COUNT_VECTORS_PATH)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading CountVectorizer features: {e}. Please ensure the feature extraction step was successful.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"CountVectorizer features not found. Please run the feature extraction step first.\")\n",
    "\n",
    "# Re-perform train-test split to ensure X_train, X_test, y_train, y_test are available\n",
    "X = count_vectors\n",
    "y = df__spam_dataset['label_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "print(\"Data split verified for parallel model training.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "## Parallel Model Training and Evaluation 🚀\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Parallel Model Training and Evaluation (Including Ensembles) ---\")\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "# Define Base Classifiers for Ensembles (to be used within Voting/Stacking)\n",
    "# These models are chosen for their diversity and common use in ensembles.\n",
    "base_clf1 = MultinomialNB()\n",
    "base_clf2 = LogisticRegression(random_state=42, solver='liblinear', max_iter=1000)\n",
    "base_clf3 = LinearSVC(random_state=42, dual=False, max_iter=1000)\n",
    "base_clf4 = RandomForestClassifier(random_state=42, n_estimators=100) # Added n_estimators\n",
    "\n",
    "# Define all models to be trained and evaluated\n",
    "models = {\n",
    "    # --- Individual Classifiers ---\n",
    "    'Multinomial Naive Bayes': base_clf1,\n",
    "    'Logistic Regression': base_clf2,\n",
    "    'Linear SVM': base_clf3,\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': base_clf4,\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Stochastic Gradient Descent (SGD)': SGDClassifier(loss='log_loss', random_state=42, max_iter=1000), # loss='log_loss' for logistic regression (probability estimates)\n",
    "    'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "\n",
    "    # --- Ensemble Methods ---\n",
    "    # Bagging: BaggingClassifier with Decision Tree base\n",
    "    'Bagging (Decision Tree)': BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(random_state=42),\n",
    "        n_estimators=100, # Number of base estimators\n",
    "        random_state=42,\n",
    "        n_jobs=-1 # Use all cores for bagging itself if possible\n",
    "    ),\n",
    "    # Boosting: AdaBoost with Decision Tree base\n",
    "    'AdaBoost (Decision Tree)': AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=1, random_state=42), # Use shallow trees (weak learners)\n",
    "        n_estimators=100, # Number of boosting stages\n",
    "        random_state=42\n",
    "    ),\n",
    "    # Voting Classifier: Combines diverse models\n",
    "    'Voting Classifier (Soft)': VotingClassifier(\n",
    "        estimators=[\n",
    "            ('mnb', base_clf1),\n",
    "            ('lr', base_clf2),\n",
    "            ('lsvc', LinearSVC(random_state=42, dual=False, max_iter=1000)) # Re-instantiate if needed, or use base_clf3\n",
    "        ],\n",
    "        voting='soft', # Use predicted probabilities\n",
    "        # weights=[0.3, 0.4, 0.3], # Example weights, can be tuned\n",
    "        n_jobs=-1 # Use all cores for voting if possible\n",
    "    ),\n",
    "    # Stacking Classifier: Meta-learner combines base predictions\n",
    "    'Stacking Classifier': StackingClassifier(\n",
    "        estimators=[\n",
    "            ('mnb', base_clf1),\n",
    "            ('lr', base_clf2),\n",
    "            ('rf', base_clf4)\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(random_state=42, solver='liblinear', max_iter=1000), # Meta-learner\n",
    "        cv=5, # Cross-validation folds for base learners' predictions\n",
    "        n_jobs=-1 # Use all cores if possible\n",
    "    )\n",
    "}\n",
    "\n",
    "# Determine the number of CPU cores to use for joblib.Parallel.\n",
    "# Using -1 means using all available CPU cores.\n",
    "# Note: Some sklearn models (like BaggingClassifier, VotingClassifier, StackingClassifier)\n",
    "# have their own `n_jobs` parameter. Setting it to -1 in their constructor handles\n",
    "# their internal parallelization. For `joblib.Parallel` here, `num_cores` controls\n",
    "# the parallel execution of the *different* models.\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Attempting to train {len(models)} models in parallel using {num_cores} cores via joblib...\")\n",
    "\n",
    "# Define the function to train and evaluate a single model.\n",
    "def train_and_evaluate_single_model(name, model, X_train_data, X_test_data, y_train_data, y_test_data):\n",
    "    \"\"\"\n",
    "    Trains a given model and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"  [Parallel] Training {name}...\")\n",
    "    try:\n",
    "        model.fit(X_train_data, y_train_data)\n",
    "        y_pred = model.predict(X_test_data)\n",
    "\n",
    "        # Calculate metrics, ensuring zero_division handles cases where a class is not predicted/present\n",
    "        accuracy = accuracy_score(y_test_data, y_pred)\n",
    "        precision = precision_score(y_test_data, y_pred, pos_label=1, zero_division=0)\n",
    "        recall = recall_score(y_test_data, y_pred, pos_label=1, zero_division=0)\n",
    "        f1 = f1_score(y_test_data, y_pred, pos_label=1, zero_division=0)\n",
    "        cm = confusion_matrix(y_test_data, y_pred, labels=np.unique(y_test_data))\n",
    "\n",
    "        return {\n",
    "            'name': name,\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  [Parallel] Error training {name}: {e}\")\n",
    "        return {\n",
    "            'name': name,\n",
    "            'model': None,\n",
    "            'accuracy': None, 'precision': None, 'recall': None, 'f1_score': None,\n",
    "            'confusion_matrix': None,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Execute training and evaluation in parallel\n",
    "parallel_results = Parallel(n_jobs=num_cores, verbose=10)(\n",
    "    delayed(train_and_evaluate_single_model)(name, model, X_train, X_test, y_train, y_test)\n",
    "    for name, model in models.items()\n",
    ")\n",
    "\n",
    "# Populate model_results dictionary\n",
    "for res in parallel_results:\n",
    "    if res['model'] is not None:\n",
    "        model_results[res['name']] = {\n",
    "            'model': res['model'],\n",
    "            'accuracy': res['accuracy'],\n",
    "            'precision': res['precision'],\n",
    "            'recall': res['recall'],\n",
    "            'f1_score': res['f1_score'],\n",
    "            'confusion_matrix': res['confusion_matrix']\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Skipping results for {res['name']} due to training error.\")\n",
    "\n",
    "print(\"\\nAll models trained and evaluated in parallel! 🎉\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "## Reviewing All Model Results\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Reviewing All Model Results ---\")\n",
    "\n",
    "if not model_results:\n",
    "    print(\"No models were successfully trained or evaluated.\")\n",
    "else:\n",
    "    # Convert results to a DataFrame for easier comparison\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Model': name,\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall'],\n",
    "            'F1-Score': results['f1_score']\n",
    "        }\n",
    "        for name, results in model_results.items() if results['accuracy'] is not None\n",
    "    ])\n",
    "    # Sort by F1-Score for better comparison in text classification\n",
    "    results_df = results_df.sort_values(by='F1-Score', ascending=False)\n",
    "\n",
    "    print(\"\\nPerformance Summary (Sorted by F1-Score):\")\n",
    "    print(results_df.to_string(index=False)) # Use to_string to avoid truncation\n",
    "\n",
    "    # Print detailed metrics for each model\n",
    "    for name, results in model_results.items():\n",
    "        print(f\"\\n--- {name} Performance ---\")\n",
    "        if results['accuracy'] is not None:\n",
    "            print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "            print(f\"  Precision: {results['precision']:.4f}\")\n",
    "            print(f\"  Recall: {results['recall']:.4f}\")\n",
    "            print(f\"  F1-Score: {results['f1_score']:.4f}\")\n",
    "            print(\"  Confusion Matrix:\")\n",
    "            unique_labels = np.unique(y_test)\n",
    "            if len(unique_labels) == 2:\n",
    "                cm = results['confusion_matrix']\n",
    "                print(f\"    True Negative (Actual: {unique_labels[0]}, Pred: {unique_labels[0]}): {cm[0, 0]}\")\n",
    "                print(f\"    False Positive (Actual: {unique_labels[0]}, Pred: {unique_labels[1]}): {cm[0, 1]}\")\n",
    "                print(f\"    False Negative (Actual: {unique_labels[1]}, Pred: {unique_labels[0]}): {cm[1, 0]}\")\n",
    "                print(f\"    True Positive (Actual: {unique_labels[1]}, Pred: {unique_labels[1]}): {cm[1, 1]}\")\n",
    "            else:\n",
    "                print(results['confusion_matrix'])\n",
    "        else:\n",
    "            print(\"  Metrics not available due to training error.\")\n",
    "\n",
    "\n",
    "# Save the entire model_results dictionary for later analysis\n",
    "try:\n",
    "    joblib.dump(model_results, MODEL_RESULTS_PATH)\n",
    "    print(f\"\\nAll model results saved to: {MODEL_RESULTS_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving all model results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A-aeP-hS5jy"
   },
   "source": [
    "##### Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8M42CqlS4mv",
    "outputId": "11749656-2d64-4123-fc04-38e506106f33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameter Tuning with GridSearchCV ---\n",
      "\n",
      "--- Tuning Logistic Regression ---\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 87\u001b[0m\n\u001b[0;32m     77\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     78\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     79\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grids[name],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Use all available CPU cores for grid search\u001b[39;00m\n\u001b[0;32m     84\u001b[0m )\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Fit GridSearchCV on the training data\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Get the best estimator found by GridSearchCV\u001b[39;00m\n\u001b[0;32m     90\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1605\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    993\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    994\u001b[0m         )\n\u001b[0;32m    995\u001b[0m     )\n\u001b[1;32m--> 997\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dipan\\miniforge3\\envs\\pytorch_xpu\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold # Added StratifiedKFold for cross-validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# --- IMPORTANT ASSUMPTION ---\n",
    "# This code block assumes that X_train, X_test, y_train, y_test are already defined\n",
    "# and available from previous execution cells (the data splitting step).\n",
    "# It also assumes 'model_results' from the previous model selection step is available\n",
    "# to help identify top performing models.\n",
    "\n",
    "# Define paths (consistent with your previous steps)\n",
    "FEATURE_EXTRACTION_DIR = 'feature_extraction_models'\n",
    "os.makedirs(FEATURE_EXTRACTION_DIR, exist_ok=True)\n",
    "TUNED_MODELS_DIR = os.path.join(FEATURE_EXTRACTION_DIR, 'tuned_models')\n",
    "os.makedirs(TUNED_MODELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# If model_results is not available, you might want to load it from where it was saved.\n",
    "# Example:\n",
    "# try:\n",
    "#     MODEL_RESULTS_PATH = os.path.join(FEATURE_EXTRACTION_DIR, 'all_model_results.joblib')\n",
    "#     model_results = joblib.load(MODEL_RESULTS_PATH)\n",
    "#     print(f\"Loaded previous model results from {MODEL_RESULTS_PATH}\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Previous model results not found. Please run model selection step first.\")\n",
    "#     # Or re-run the model selection step if you need to proceed\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "## Hyperparameter Tuning with GridSearchCV ⚙️\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Hyperparameter Tuning with GridSearchCV ---\")\n",
    "\n",
    "# Select a few top-performing models for tuning based on previous F1-scores\n",
    "# It's recommended to choose 2-3 models that showed promising results.\n",
    "# For demonstration, let's select Logistic Regression and Linear SVM as common strong performers for text.\n",
    "# You can add or change these based on the actual results from the previous step.\n",
    "models_to_tune = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, solver='liblinear', max_iter=1000),\n",
    "    'Linear SVM': LinearSVC(random_state=42, dual=False, max_iter=1000),\n",
    "    # 'Random Forest': RandomForestClassifier(random_state=42), # Can also be tuned, but takes longer\n",
    "    # 'Multinomial Naive Bayes': MultinomialNB(), # Often less parameters to tune\n",
    "}\n",
    "\n",
    "# Define hyperparameter grids for each selected model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10, 100],  # Inverse of regularization strength\n",
    "        'penalty': ['l1', 'l2']  # Regularization type\n",
    "    },\n",
    "    'Linear SVM': {\n",
    "        'C': [0.1, 1, 10],      # Regularization parameter\n",
    "        # 'loss': ['hinge', 'squared_hinge'] # Can also be tuned, but default is usually good\n",
    "    },\n",
    "    # 'Random Forest': {\n",
    "    #     'n_estimators': [50, 100, 200], # Number of trees in the forest\n",
    "    #     'max_depth': [10, 20, None],   # Maximum depth of the tree\n",
    "    #     'min_samples_split': [2, 5]    # Minimum number of samples required to split an internal node\n",
    "    # }\n",
    "}\n",
    "\n",
    "tuned_model_results = {}\n",
    "# Using StratifiedKFold for robust cross-validation, especially important for imbalanced data\n",
    "cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models_to_tune.items():\n",
    "    if name in param_grids:\n",
    "        print(f\"\\n--- Tuning {name} ---\")\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grids[name],\n",
    "            cv=cv_folds, # Use stratified cross-validation\n",
    "            scoring='f1', # Optimize for F1-score (good balance of precision/recall)\n",
    "            verbose=2, # Print detailed progress\n",
    "            n_jobs=-1 # Use all available CPU cores for grid search\n",
    "        )\n",
    "\n",
    "        # Fit GridSearchCV on the training data\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best estimator found by GridSearchCV\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "\n",
    "        print(f\"\\nBest parameters for {name}: {best_params}\")\n",
    "        print(f\"Best F1-score on training data (cross-validation): {best_score:.4f}\")\n",
    "\n",
    "        # Evaluate the best model on the unseen test set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "\n",
    "        print(f\"\\n{name} Performance on Test Set (after tuning):\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1-Score: {f1:.4f}\")\n",
    "        print(f\"  Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "        # Store results\n",
    "        tuned_model_results[name] = {\n",
    "            'model': best_model,\n",
    "            'best_params': best_params,\n",
    "            'cv_f1_score': best_score,\n",
    "            'test_accuracy': accuracy,\n",
    "            'test_precision': precision,\n",
    "            'test_recall': recall,\n",
    "            'test_f1_score': f1,\n",
    "            'test_confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "        # Save the best model\n",
    "        model_filename = os.path.join(TUNED_MODELS_DIR, f'{name.lower().replace(\" \", \"_\")}_tuned_model.joblib')\n",
    "        try:\n",
    "            joblib.dump(best_model, model_filename)\n",
    "            print(f\"Tuned {name} model saved to: {model_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving tuned {name} model: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"No hyperparameter grid defined for {name}. Skipping tuning for this model.\")\n",
    "\n",
    "print(\"\\nHyperparameter tuning complete! 🎉\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "## Summary of Tuned Model Results\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Summary of Tuned Model Results ---\")\n",
    "if not tuned_model_results:\n",
    "    print(\"No models were tuned or evaluated.\")\n",
    "else:\n",
    "    tuned_results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Model': name,\n",
    "            'Best Params': str(results['best_params']),\n",
    "            'CV F1-Score (Train)': results['cv_f1_score'],\n",
    "            'Test Accuracy': results['test_accuracy'],\n",
    "            'Test Precision': results['test_precision'],\n",
    "            'Test Recall': results['test_recall'],\n",
    "            'Test F1-Score': results['test_f1_score']\n",
    "        }\n",
    "        for name, results in tuned_model_results.items()\n",
    "    ])\n",
    "    tuned_results_df = tuned_results_df.sort_values(by='Test F1-Score', ascending=False)\n",
    "    print(tuned_results_df.to_string(index=False))\n",
    "\n",
    "    # Save all tuned model results summary\n",
    "    TUNED_RESULTS_SUMMARY_PATH = os.path.join(TUNED_MODELS_DIR, 'tuned_model_summary.joblib')\n",
    "    try:\n",
    "        joblib.dump(tuned_model_results, TUNED_RESULTS_SUMMARY_PATH)\n",
    "        print(f\"\\nSummary of tuned model results saved to: {TUNED_RESULTS_SUMMARY_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving tuned model results summary: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNAhiF5YXx8itWe6H+P20XB",
   "collapsed_sections": [
    "DOL-zuDFA0k3",
    "uzDvBmA7E8Ug",
    "9K8NMzprI9ZI",
    "xfST8CO0_0Wn",
    "Rii-udbgS1ly",
    "3A-aeP-hS5jy"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_xpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
